import base64
import datetime
import hashlib
import logging
import os
import pickle
import random
import socket
import time

import mccache as mc

# Callback method for key that are updated within 1 second of lookup in the background.
#
def change(ctx: dict):
    """Callback method to be notified of changes withion one second of previous lookup.

    Args:
        ctx :dict   A context dictionary of the following format:
                        {'key': key ,'lkp': lkp ,'tsm': tsm ,'prvcrc': old ,'newcrc': new}
                    If 'prvcrc' is None then it is a insertion.
                    If 'newcrc' is None then it is a deletion.
    """
    elapse = round((ctx['tsm'] - ctx['lkp']) / mc.ONE_NS_SEC ,4)
    if  ctx['newcrc'] is None:
        mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.FYI ,tsm=time.time_ns() ,nms=cache.name ,key=ctx['key'] ,crc=ctx['newcrc'] ,msg=f")>   FYI {key} got deleted within {elapse} sec in the background." )
    else:
        mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.FYI ,tsm=time.time_ns() ,nms=cache.name ,key=ctx['key'] ,crc=ctx['newcrc'] ,msg=f")>   FYI {key} got updated within {elapse} sec in the background." )


# Initialization section.
#
rndseed = 17
if 'TEST_RANDOM_SEED' in os.environ:
    rndseed = int(os.environ['TEST_RANDOM_SEED'])
else:
    rndseed = int(str(socket.getaddrinfo(socket.gethostname() ,0 ,socket.AF_INET )[0][4][0]).split(".")[3])
random.seed( rndseed )

sleepspan = 100 # 1000 = 1sec
if 'TEST_SLEEP_SPAN'  in os.environ:
    sleepspan = int(os.environ['TEST_SLEEP_SPAN'])    # In seconds.

sleepunit = 100 # 1000 = 0.001s ,100 = 0.01s ,10 = 0.1s ,1 = 1s
if 'TEST_SLEEP_UNIT'  in os.environ:
    sleepunit = int(os.environ['TEST_SLEEP_UNIT'])

entries = 100
if 'TEST_MAX_ENTRIES' in os.environ:
    entries = int(os.environ['TEST_MAX_ENTRIES'])

duration = 5    # Five minute.
if 'TEST_RUN_DURATION' in os.environ:
    duration = int(os.environ['TEST_RUN_DURATION'])  # In minutes.

NEXT_SSEC = 5   # Synchronize seconds.
cache = mc.get_cache()
bgn = time.time()
end = time.time()


# Random test section.
#
mc.logger.info(f"{mc.SRC_IP_ADD} Test config: Seed={rndseed:3} ,Duration={duration:3} min ,Keys={entries:3} ,Span={sleepspan:3} ,Unit={sleepunit:3}")

lookuptsm = dict()
ctr:int = 0     # Counter
while (end - bgn) < (duration*60):  # Seconds.
    time.sleep( random.randint(1 ,sleepspan) / sleepunit )

    if  random.randint(0 ,20) % 5 == 0: # Arbitarily.
        # Keys unique to this node.
        key = f'K{mc.SRC_IP_SEQ:03}-{int((time.time_ns()/100) % entries):04}'
    else:
        # Keys can be generated by every nodes.
        key = f'K000-{int((time.time_ns()/100) % entries):04}'

    # DEBUG trace.
    if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.BASIC:
        # Since the last read, did the value got updated?  If so, how long after the read?
        if  key in lookuptsm and key in cache.metadata:
            if (lookuptsm[ key ]['tsm'] < cache.metadata[ key ]['tsm']) and (lookuptsm[ key ]['crc'] != cache.metadata[ key ]['crc']):
                dur = round((cache.metadata[ key ]['tsm'] - lookuptsm[ key ]['tsm']) / mc.ONE_NS_SEC ,4)
                crc = cache.metadata[ key ]['crc']
                mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.WRN ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f">   Value changed within {dur:0.5f} sec after lookup." )
                del lookuptsm[ key ]

    ctr +=  1
    opc =   random.randint( 0 ,21 ) # Generate a range of 20  values.
    match   opc:
        case 13|17:     # NOTE: 10% are deletes.
            if  key in cache:
                #crc =  cache.getmeta( key )['crc']
                # For PyCache
                crc =  cache.metadata[ key ]['crc']

                # DEBUG trace.
                if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.EXTRA:
                    mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   DEL {key} in test script." )

                # Evict cache.
                try:
                    del cache[ key ]
                except KeyError:
                    # DEBUG trace.
                    if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.BASIC:
                        if  key in lookuptsm:
                            dur = round((time.time_ns() - lookuptsm[ key ]['tsm']) / mc.ONE_NS_SEC ,4)
                            mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=None ,msg=f">   Value changed within {dur:0.5f} sec after lookup." )

                # Cache entry was just deleted, evict the lookup tsm.
                if  key in lookuptsm:
                    del lookuptsm[ key ]

                # DEBUG trace.
                if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.SUPERFLOUS:
                    if  key in cache:
                        mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   ERR:{key} still persist in cache in test script!" )
                    else:
                        mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   OK: {key} deleted from cache in test script." )
        case 0|1|2|3:   # NOTE: 20% are inserts.
            if  key not in cache:
                val = (mc.SRC_IP_SEQ ,datetime.datetime.now(datetime.UTC) ,ctr) # The mininum fields to totally randomize the value.
                pkl: bytes = pickle.dumps( val )
                crc: str   = base64.b64encode( hashlib.md5( pkl ).digest() ).decode()  # noqa: S324

                # DEBUG trace.
                if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.SUPERFLOUS:
                    mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   INS {key}={val} in test script." )

                # Insert cache.
                cache[ key ] = val

                # DEBUG trace.
                if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.SUPERFLOUS:
                    if  key not in cache:
                        mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   ERR:{key} NOT persisted in cache in test script!" )
                    else:
                        if  val != cache[ key ]:
                            mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   ERR:{key} value is incoherent in cache in test script!" )
                        else:
                            mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   OK: {key} persisted in cache in test script." )
        case 4|5|6|7|8|9:   # NOTE: 30% are updates.  Simulate much more updates than inserts.
            if  key in cache:
                val = (mc.SRC_IP_SEQ ,datetime.datetime.now(datetime.UTC) ,ctr) # The mininum fields to totally randomize the value.
                pkl: bytes = pickle.dumps( val )
                crc: str   = base64.b64encode( hashlib.md5( pkl ).digest() ).decode()  # noqa: S324

                # DEBUG trace.
                if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.SUPERFLOUS:
                    mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.DEL ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   UPD {key}={val} in test script." )

                # Update cache.
                cache[ key ] = val

                # Cache entry was just updated, evict the lookup tsm.
                if  key in lookuptsm:
                    del lookuptsm[ key ]

                # DEBUG trace.
                if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.SUPERFLOUS:
                    if  key not in cache:
                        mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.UPD ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   ERR:{key} NOT persisted in cache in test script!" )
                    else:
                        if  val != cache[ key ]:
                            mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.UPD ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   ERR:{key} value is incoherent in test script!" )
                        else:
                            mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.UPD ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=crc ,msg=f")>   OK: {key} persisted in cache in test script!" )
        case _:     # NOTE: 40% are lookups.
            # Look up cache.
            val = cache.get( key ,None )

            if  val:
                if  key not in lookuptsm:
                    lookuptsm[ key ] = {}
                lookuptsm[ key ]['tsm'] = time.time_ns()
                lookuptsm[ key ]['crc'] = cache.metadata[ key ]['crc']

            # DEBUG trace.
            if  mc._mcConfig.debug_level >= mc.McCacheDebugLevel.SUPERFLOUS:
                if  not val:
                    mc._log_ops_msg( logging.DEBUG ,opc=mc.OpCode.INQ ,tsm=time.time_ns() ,nms=cache.name ,key=key ,crc=None ,msg=f")>   WRN:{key} value is None in test script!")

    end = time.time()

# Wait for all members to catch up before existing together.
bgn = time.time()
bgnsec = time.localtime().tm_sec
time.sleep(NEXT_SSEC +(NEXT_SSEC - (bgnsec % NEXT_SSEC)))   # Try to get the cluster to stop at the same second to reduce RAK.
end = time.time()

# Format ouput to be consistent with the McCache log format.
mc.logger.info(f"{mc.SRC_IP_ADD} Done testing. Querying final cache checksum.")

mc.get_cache_checksum( cache.name ) # Query the cache at exit.

tsm = cache.TSM_VERSION()
tsm = f"{time.strftime('%H:%M:%S' ,time.gmtime(tsm//100_000_000))}.{tsm%100_000_000}"
mc.logger.info(f"{mc.SRC_IP_ADD} Exiting at {tsm}")
